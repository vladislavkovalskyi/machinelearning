Ниже приведены примерные вопросы из методички (которые часто встречаются при выполнении подобного задания с датасетом Adult) и ответы к ним на основе вашего решения.
Если формулировки вопросов немного отличаются от тех, что в вашей методичке, подстройте ответы под конкретную формулировку.

1. Сколько объектов и сколько признаков в данных? Есть ли дубликаты?

Ответ:
	•	После чтения исходного датасета adult.data мы получили примерно 32561 объектов (строк).
	•	Число исходных столбцов (признаков) — 15 (если считать вместе с таргетом income), но после кодирования One-Hot их стало больше (каждая категориальная колонка развернулась в несколько).
	•	Дубликаты специально не выявлялись, но обычно в наборе Adult явных полных дублей нет.

2. Есть ли пропуски в данных? Как вы с ними обошлись?

Ответ:
	•	Пропуски в датасете присутствуют, они изначально обозначены ? в тексте файла. При чтении мы заменили их на NaN, а затем посмотрели через df.isnull().sum().
	•	Заполнение пропусков:
	•	Для числовых столбцов взяли медиану (устойчиво к выбросам).
	•	Для категориальных — моду (наиболее часто встречающееся значение).

3. Какие признаки числовые, какие категориальные? Какой метод кодирования использовали?

Ответ:
	•	Числовые (количественные) столбцы в Adult: age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week.
	•	Категориальные (номинальные) столбцы: workclass, marital-status, occupation, relationship, race, sex, native-country, а также целевой признак income.
	•	Для бинаризации целевого признака income использовали LabelEncoder (перевод <=50K в 0, >50K в 1).
	•	Для остальных категориальных столбцов применили One-Hot-Encoding (pd.get_dummies).

4. Зачем делали масштабирование (MinMaxScaler)?

Ответ:
	•	Масштабирование (приведение к диапазону [0, 1]) особенно важно для методов, основанных на расстоянии (например, k-NN). Там масштабы разных признаков сильно влияют на результат.
	•	Также логистическая регрессия может сходиться быстрее и стабильнее, если признаки имеют сопоставимые значения.

5. Есть ли в данных дисбаланс классов и как он учитывается при оценке?

Ответ:
	•	Класс <=50K встречается примерно в 75% случаев, >50K — в 25%. Дисбаланс не катастрофический, но ощутимый.
	•	Для учёта дисбаланса мы смотрим не только Accuracy, но и AUC-ROC, Precision, Recall и F1.

6. Какую основную метрику выбрали и почему?

Ответ:
	•	В качестве основной выбрали AUC-ROC, так как она хорошо отражает способность модели различать классы и менее чувствительна к перекосу классов, чем простая точность.
	•	Плюс дополнительно смотрели Accuracy, Precision, Recall, F1 для более детального анализа.

7. Почему важен Train/Test Split? Какое соотношение выбрано?

Ответ:
	•	Разделение на обучающую и тестовую выборки (80% / 20%) нужно для того, чтобы мы могли честно оценить качество на «неизвестных» данных.
	•	Использовали stratify=y для сохранения пропорций классов.

8. Как обучали логистическую регрессию без регуляризации? Какие выводы?

Ответ:
	•	В sklearn по умолчанию включена L2-регуляризация, поэтому для отключения мы указали penalty='none'.
	•	Обучили на тренировочных данных, посмотрели на метрики: модель показала Accuracy ~85%, AUC ~0.90+ (примерно).
	•	Небольшой разрыв между метриками на train и test всё же есть, что указывает на небольшое переобучение.

9. Зачем нужна кросс-валидация и что показали её результаты?

Ответ:
	•	Кросс-валидация (например, 5-фолд) помогает получить более устойчивую оценку качества модели, сглаживая случайность одного-единственного разбиения.
	•	Мы увидели, что средние значения по фолдам соотносятся с результатами на тестовой выборке, а стандартные отклонения невелики. Значит, модель довольно стабильна.

10. Зачем нужна регуляризация (L2) в логистической регрессии? Как выбрали оптимальное C?

Ответ:
	•	Регуляризация позволяет избежать переобучения, «сжимая» веса модели.
	•	В LogisticRegression гиперпараметр C обратно пропорционален силе регуляризации: чем меньше C, тем сильнее штраф.
	•	Подобрали лучшее значение C (из [0.01, 0.1, 1, 10, 100, 1000]) через GridSearchCV с метрикой AUC. Оказалось, что оптимальное C ~ 10 или 100 (в вашем решении).
	•	Модель с регуляризацией показала чуть лучший AUC и более гладкие веса, чем без неё.

11. Какие ещё методы для отбора признаков рассматривались?

Ответ:
	•	Упоминался подход с L1-регуляризацией (где некоторые коэффициенты обнуляются). Это может выполнять отбор признаков автоматически.
	•	Также можно использовать SelectKBest, RFE или другие методы, но в данном решении это было дополнительным (необязательным) пунктом.

12. Какой дополнительный классификатор обучался и как он справился с задачей?

Ответ:
	•	Обучался k-NN (метод ближайших соседей).
	•	Для оптимизации подбирали n_neighbors (3, 5, 15, 50, 100) и weights (uniform, distance) через GridSearchCV. Лучшим оказался, например, n_neighbors=50, weights='uniform'.
	•	Результаты k-NN: Accuracy ~83–84%, AUC ~0.88. Чуть хуже, чем у логистической регрессии.

13. Какая модель оказалась лучше и почему?

Ответ:
	•	Сравнение финальных метрик на тесте показало, что логистическая регрессия (особенно с оптимальным C) даёт AUC ~0.90+ и точность около 85%, тогда как k-NN — AUC ~0.88, точность ~83–84%.
	•	Поэтому логистическая регрессия в данном случае побеждает по большинству ключевых метрик.

Итоговый вывод

На основе вашего решения, можно сделать вывод, что:
	1.	Предобработка и корректное заполнение пропусков имеют первостепенное значение.
	2.	AUC-ROC и другие метрики показывают, что дисбаланс классов не слишком сильный, но его надо учитывать (особенно при выборе метрики).
	3.	Логистическая регрессия, особенно с правильным подбором гиперпараметра C, показала себя лучше k-NN.
	4.	Кросс-валидация и регуляризация улучшили устойчивость и качество модели.

Таким образом, логистическая регрессия при оптимальном C становится лучшим выбором для данной задачи классификации.